{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Classification\n",
    "#### 23518008 Hidayaturrahaman & 23518032 Henry Menori\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Melakukan klasifikasi pada tweet kedalam class Keluhan, Respon, atau Bukan Keluhan/Respon.\n",
    "\n",
    "NLP S2 IF-ITB 2018/2019\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Keluhan</th>\n",
       "      <th>Respon</th>\n",
       "      <th>Bukan Keluhan/Respon</th>\n",
       "      <th>Topik Umum</th>\n",
       "      <th>Topik Spesifik</th>\n",
       "      <th>Lokasi</th>\n",
       "      <th>Waktu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>678844958237822977</td>\n",
       "      <td>21 Dec 2015, 02:50:25 PM</td>\n",
       "      <td>@EL_Atheos @ridwankamil ya mungkin karena pere...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Bukan Keluhan</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>678845619444703232</td>\n",
       "      <td>21 Dec 2015, 02:53:02 PM</td>\n",
       "      <td>@ridwankamil @dbmpkotabdg kang teman saya tert...</td>\n",
       "      <td>Ya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lingkungan Hidup</td>\n",
       "      <td>pohon tumbang</td>\n",
       "      <td>Jalan Sangkuriang depan Polsek Coblong</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>678846848619024384</td>\n",
       "      <td>21 Dec 2015, 02:57:55 PM</td>\n",
       "      <td>Di tribun jabar biasanya suka di post agenda k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Bukan Keluhan</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>678847097047678977</td>\n",
       "      <td>21 Dec 2015, 02:58:54 PM</td>\n",
       "      <td>@dbmpkotabdg RT @fajriattack: Lapor pak @ridwa...</td>\n",
       "      <td>Ya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Infrastruktur</td>\n",
       "      <td>lampu penerangan jalan umum</td>\n",
       "      <td>Jalan depan Kampus LPKIA</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>678849085621714944</td>\n",
       "      <td>21 Dec 2015, 03:06:49 PM</td>\n",
       "      <td>@diskamtam bapak/ibu mau tanya, kalo pemelihar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ya</td>\n",
       "      <td>Bukan Keluhan</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                 Timestamp  \\\n",
       "0  678844958237822977  21 Dec 2015, 02:50:25 PM   \n",
       "1  678845619444703232  21 Dec 2015, 02:53:02 PM   \n",
       "2  678846848619024384  21 Dec 2015, 02:57:55 PM   \n",
       "3  678847097047678977  21 Dec 2015, 02:58:54 PM   \n",
       "4  678849085621714944  21 Dec 2015, 03:06:49 PM   \n",
       "\n",
       "                                               Tweet Keluhan Respon  \\\n",
       "0  @EL_Atheos @ridwankamil ya mungkin karena pere...     NaN    NaN   \n",
       "1  @ridwankamil @dbmpkotabdg kang teman saya tert...      Ya    NaN   \n",
       "2  Di tribun jabar biasanya suka di post agenda k...     NaN    NaN   \n",
       "3  @dbmpkotabdg RT @fajriattack: Lapor pak @ridwa...      Ya    NaN   \n",
       "4  @diskamtam bapak/ibu mau tanya, kalo pemelihar...     NaN    NaN   \n",
       "\n",
       "  Bukan Keluhan/Respon        Topik Umum               Topik Spesifik  \\\n",
       "0                   Ya     Bukan Keluhan                            -   \n",
       "1                  NaN  Lingkungan Hidup                pohon tumbang   \n",
       "2                   Ya     Bukan Keluhan                            -   \n",
       "3                  NaN     Infrastruktur  lampu penerangan jalan umum   \n",
       "4                   Ya     Bukan Keluhan                            -   \n",
       "\n",
       "                                   Lokasi Waktu   \n",
       "0                                       -      -  \n",
       "1  Jalan Sangkuriang depan Polsek Coblong      -  \n",
       "2                                       -      -  \n",
       "3                Jalan depan Kampus LPKIA      -  \n",
       "4                                       -      -  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Data:7527, keluhan:2744, respon:917, other:3866\n"
     ]
    }
   ],
   "source": [
    "keluhan = 0\n",
    "respon = 0\n",
    "other = 0\n",
    "\n",
    "for data in dataY:\n",
    "    if data[0]==1: \n",
    "        keluhan += 1\n",
    "    elif data[1]==1:\n",
    "        respon += 1\n",
    "    else:\n",
    "        other += 1\n",
    "    \n",
    "print(\" Total Data:{}, keluhan:{}, respon:{}, other:{}\".format(dataY.shape[0], keluhan, respon, other))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ======== Preprocessing Data ===============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.csv', sep=';')\n",
    "\n",
    "dataClean = data[data.columns[2:6]]\n",
    "\n",
    "dataX = dataClean[dataClean.columns[0]].values\n",
    "dataY = dataClean[dataClean.columns[1:]].values\n",
    "\n",
    "# ==== one hot encode data Y ========\n",
    "\n",
    "for i in range(len(dataY)):\n",
    "    for j in range(len(dataY[i])):\n",
    "        if dataY[i][j] == 'Ya':\n",
    "            dataY[i][j] = 1\n",
    "        else:\n",
    "            dataY[i][j] = 0\n",
    "            \n",
    "# ==== tokenization data X ==========\n",
    "\n",
    "for i in range(len(dataX)):\n",
    "    dataX[i] = dataX[i].split()\n",
    "    \n",
    "import re\n",
    "\n",
    "# cleaning from symbol, link, and emoticon\n",
    "\n",
    "for i in range(len(dataX)):\n",
    "    for j in range(len(dataX[i])):\n",
    "        \n",
    "        dataX[i][j] = re.sub(\"\\W\", \" \", dataX[i][j])\n",
    "        dataX[i][j] = re.sub(\"_\", \" \", dataX[i][j])\n",
    "        \n",
    "        #coba hapus semua yang ada angkanyaaa\n",
    "        \n",
    "        if re.search(r\"\\bhttp\\w+\", dataX[i][j]) != None:\n",
    "            dataX[i][j] = \"\"\n",
    "        \n",
    "        if re.search(r\"\\d\", dataX[i][j]) != None:\n",
    "            dataX[i][j] = \"\"\n",
    "        \n",
    "        \n",
    "            \n",
    "# Concating for normalizing word purpose\n",
    "\n",
    "for i in range(len(dataX)):\n",
    "    dataX[i] = ' '.join(dataX[i])\n",
    "    dataX[i] = dataX[i].split()\n",
    "    dataX[i] = ' '.join(dataX[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize word using prosa.ai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adiraksa adit aditsap77488878 adiwiyata adji adjie adl adlah adlh admin administrasi adminnya admnya adrianmpriyatna adsastrawidjaja adu aduan aduh aduhh adukan adukeun aduuh advent ady ae aendiri aer afansobirin affan afiat afrika afrikaa after aga agak agama agar agenda ageng ageung ageungan agkt agm agma agr agung agus agustus ah ahaaaaay\n",
      "50\n",
      "['adiraksa', 'adit', 'aditsap77488878', 'adiwiyata', 'adji', 'adjie', 'adalah', 'adlah', 'adalah', 'admin', 'administrasi', 'adminnya', 'adalahnya', 'adrianmpriyatna', 'adsastrawidjaja', 'adu', 'aduan', 'aduh', 'aduh', 'adukan', 'adukeun', 'aduuh', 'advent', 'ady', 'ae', 'aendiri', 'aer', 'afansobirin', 'affan', 'afiat', 'afrika', 'afrikaa', 'after', 'agak', 'agak', 'agama', 'agar', 'agenda', 'ageng', 'ageung', 'ageungan', 'angkot', 'agm', 'agma', 'agr', 'agung', 'agus', 'agustus', 'ah', 'alay']\n"
     ]
    }
   ],
   "source": [
    "# ====== Normalize word using prosa.ai API =============\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.prosa.ai/v1/normals\"\n",
    "\n",
    "# payload = \"{\\n  \\\"text\\\": \\\"EL Atheos ridwankamil ya mungkin karena perempuan bj merah yg paling kelihatan karena berada di depan laki2 jg sering jadi bahan joke RK#ridwankamil dbmpkotabdg kang teman saya tertimpa pohn dijln sangkuriang dpn polsek coblong tlg ditertibkan phn yg sdh lapuknuhun#Di tribun jabar biasanya suka di post agenda kang emil Tapi gak setiap hari sih RT Ry ory Klo saya k Bdg bs ketemu Kang ridwankamil dmn#dbmpkotabdg RT fajriattack Lapor pak ridwankamil lampu penerangan jalan depan kampus lpkia ngaplek#diskamtam bapak ibu mau tanya kalo pemeliharaan taman2 yg banyak dibagun skr gimana nantinya#Pa ridwankamil tempat kerja saya tgl 25 tetep masuk gimana itu aturannya pa kalo misalnya ga masuk dipotong gaji#pdamtirtawening ridwankamil 00106700466 punten sudah seminggu air PDAM tidak mengalir Mohon bantuannya segera#RT KendiAdiputra Anak SMA yang ketahuan mencuri helem di bip hari ini pak ridwankamil#pdamtirtawening ternyata sudah ada warga yg konfirm ke pdam katanya air tidak akan mengalir selama 1 bulan krn ada perbaikan ridwankamil#Kang ridwankamil jalan di Muararajeun Baru baru sebulanan dibenerin udah rusak lagi Yang ngerjain kaya ga serius#RT AdamMaulud Didinyamah kieu bosss Kumaha pak ridwankamil daerah sapharuhai yeuh kang parkir liar seeur pisan atuhda#Pak ridwankamil punten tolong marahin motor yg ga pake spakbor belakang Udah mah td kehujanan wajah rupawan sy kacepretan#dbmpkotabdg ridwankamil hatur nuhun dbmp lampu jl merdeka atos sae#Kalo PemkotBandung ridwankamil mau batasi kendaraan yg feasible dan bisa dilakukan segera dlm wewenang mrk berantas parkir badan jalan#RT iqbalfg Pak ridwankamil punten tolong marahin motor yg ga pake spakbor belakang Udah mah td kehujanan wajah rupawan sy kacepretan#Pak tadi saya habis dari Kebun Binatang itu kebersihannya masih kurang dan orangutannya juga jomblo pak mohon solusi pak ridwankamil#RT TadjiUINBdg ridwankamil lapor pak mobil ni 2x buang sampah ke pinggir jalan sepanjang jalan sersan bajuri no 24#Bonbin bandung agak kurang terawat kasian sama hewan hewannya Pak ridwankamil coba bonbin bandung di bikin kayak singapore zoo#Pak ridwankamil sy pnya saran angkot 08 dan Antapani yg Coklat dilebur sj tp arah trayeknya dibagi 2 kyk Cimbeuleuit dl#RT TadjiUINBdg ridwankamil lapor pak mobil ni 2x buang sampah ke pinggir jalan sepanjang jalan sersan bajuri no 24#ridwankamil pak sya lg mau pulang naek diangkot cheum ledeng ad ibu2 sma bpak2 ngmongin bpak tp tenang koq pak ngmongin nya yg baik2#dishub kotabdg sepertinya yang membuat jlnan macet adlah prmptan Hrsnya ada jambtn lyang ditiap perempatan ridwankamil#infobdg kalau macet begini di lihat di depannya karena apa macetnya angkot bus ngetem pasti ada sumber titik macetnya ridwankamil#SaurWargi reckythea tolg ad seorang wanita d jl eikman tiap pg jam 9 suka loncat k tngh jln ambil kunci mtr dgn tebusan 10rb Satpolppbdg#BimaAryaS DiskominfoBdg dishub jabar dishub kotabdg mendukung rencana badan hukum angkot jgn sampai kasus metromini tjd nanti di bgr#RT se bdg SaurWargi reckythea tolg ad seorang wanita d jl eikman tiap pg jam 9 suka loncat k tngh jln ambil kunci mtr dgn tebusan 10rb#EL Atheos hendronurkholis ridwankamil he he perempuan itu sebelumnya dh ditanya sama rk dah nikah pa masih single#Punten saya baru denger beritanya apa sudah ditertibkan modus mengambil kunci motor Satpolppbdg PRFMnews#Min dbmpkotabdg kalau dilihat dari foto di bawah ini sebenarnya Jl Cemara memiliki trotoar ga ya Tx#RT soowooBDG Min dbmpkotabdg kalau dilihat dari foto di bawah ini sebenarnya Jl Cemara memiliki trotoar ga ya Tx#ridwankamil Satpolppbdg Mohon diamankan Pa Ogah per4an Pasar Suci Cihaurgeulis membuat jalan Suci macet total amp kacau Nuhun PERHATIANA#ridwankamil dbmpkotabdg Mohon dipasang tiang utk LAMPU MERAH DI PER4AN PASAR SUCI CIHAURGEULIS SERING TERJADI MACET TOTAL IWGinting#Setuju ini sepanjang jl riau banyak yg parkir Malah ada yg di trotoar juga ridwankamil#pjudbmpbdg ass pak kiaracondong masih bnyk lampu jalan yg mati bbk sari 1mohon di perbaiki Kec Kircon Kel BabakanSari ridwankamil#Pak wali upami angkot di bandung kumaha nasibna Di jakarta metromini tos jadi korban ridwankamil#RT TadjiUINBdg ridwankamil lapor pak mobil ni 2x buang sampah ke pinggir jalan sepanjang jalan sersan bajuri no 24\\\"\\n}\"\n",
    "text = ' '.join(chars[400:450])\n",
    "print(text)\n",
    "payload = json.dumps({'text': text})\n",
    "headers = {\n",
    "    'Content-Type': \"application/json\",\n",
    "    'x-api-key': \"shOC0xYA3hoZG6SZ50M2eucwiH1jyFBlzltY3kSD\"\n",
    "#     'Postman-Token': \"f6cd5852-6186-4e19-b700-5a0e79ba4d41\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
    "\n",
    "normal = json.loads(response.text)\n",
    "charsNormal = normal['text'].split(' ')\n",
    "\n",
    "print(len(charsNormal))\n",
    "print(charsNormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2d7c3a7179ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcharsNormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharsNormal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharsNormal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "normal = json.loads(response.text)\n",
    "charsNormal = normal['text'].split(' ')\n",
    "\n",
    "print(len(charsNormal))\n",
    "print(charsNormal[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridwankamil dbmpkotabdg kang teman saya tertimpa pohn dijln sangkuriang dpn polsek coblong tlg ditertibkan phn yg sdh lapuknuhun\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataX[1])\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for trainin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words:  121911\n",
      "total Vocab:  10064\n"
     ]
    }
   ],
   "source": [
    "corpus = ' '.join(dataX).lower().split()\n",
    "\n",
    "chars = sorted(list(set(corpus)))\n",
    "char_to_int = dict((c,i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i,c) for i, c in enumerate(chars))\n",
    "\n",
    "import pickle \n",
    "\n",
    "pickle_out = open(\"dict.pickle\",\"wb\")\n",
    "pickle.dump(char_to_int, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "\n",
    "n_chars = len(corpus)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Words: \", n_chars)\n",
    "print (\"total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataX)):\n",
    "    dataX[i] = [char_to_int[char] for char in dataX[i].lower().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7527,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "- Naive Bayes\n",
    "- Decission Tree\n",
    "- Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = numpy.load('embeddedWord.npz')\n",
    "dataX = data['dataX']\n",
    "dataY = data['dataY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "dataY_svm = []\n",
    "for i in range(len(dataY)):\n",
    "    dataY_svm.append(numpy.argmax(dataY[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9800929  0.96480744 0.96212625 0.95282392 0.78058511] 0.928087121843171\n"
     ]
    }
   ],
   "source": [
    "# ==============     SVM    ======================\n",
    "\n",
    "\n",
    "\n",
    "# Train model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "score = cross_val_score(model, dataX, dataY_svm, scoring='accuracy', cv=5 )\n",
    "print(score, numpy.mean(score))\n",
    "\n",
    "# model.fit(dataX, dataY_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29794293 0.29415671 0.23388704 0.26445183 0.25731383] 0.2695504679411722\n"
     ]
    }
   ],
   "source": [
    "# =============== Naive Bayes ===================\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "score = cross_val_score(model, dataX, dataY_svm, scoring='accuracy', cv=5)\n",
    "print(score, numpy.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89847379 0.90438247 0.86245847 0.9089701  0.74800532] 0.8644580299363535\n"
     ]
    }
   ],
   "source": [
    "# =============== Random Forest ===================\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "score = cross_val_score(model, dataX, dataY_svm, scoring='accuracy', cv=5)\n",
    "print(score, numpy.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 31, 32)            322048    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 250)               248250    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 595,701\n",
      "Trainable params: 595,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayatura/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6021 samples, validate on 1506 samples\n",
      "Epoch 1/10\n",
      "6021/6021 [==============================] - 3s 502us/step - loss: 0.5577 - acc: 0.6788 - val_loss: 0.4833 - val_acc: 0.7253\n",
      "Epoch 2/10\n",
      "6021/6021 [==============================] - 2s 314us/step - loss: 0.4542 - acc: 0.7526 - val_loss: 0.4164 - val_acc: 0.7629\n",
      "Epoch 3/10\n",
      "6021/6021 [==============================] - 2s 359us/step - loss: 0.3745 - acc: 0.8149 - val_loss: 0.3773 - val_acc: 0.8021\n",
      "Epoch 4/10\n",
      "6021/6021 [==============================] - 2s 298us/step - loss: 0.2736 - acc: 0.8783 - val_loss: 0.3318 - val_acc: 0.8663\n",
      "Epoch 5/10\n",
      "6021/6021 [==============================] - 2s 347us/step - loss: 0.2045 - acc: 0.9152 - val_loss: 0.3462 - val_acc: 0.8590\n",
      "Epoch 6/10\n",
      "6021/6021 [==============================] - 2s 353us/step - loss: 0.1434 - acc: 0.9440 - val_loss: 0.3904 - val_acc: 0.8251\n",
      "Epoch 7/10\n",
      "6021/6021 [==============================] - 2s 312us/step - loss: 0.1077 - acc: 0.9585 - val_loss: 0.3665 - val_acc: 0.8712\n",
      "Epoch 8/10\n",
      "6021/6021 [==============================] - 2s 348us/step - loss: 0.0844 - acc: 0.9685 - val_loss: 0.4802 - val_acc: 0.8132\n",
      "Epoch 9/10\n",
      "6021/6021 [==============================] - 2s 308us/step - loss: 0.0650 - acc: 0.9781 - val_loss: 0.3929 - val_acc: 0.8710\n",
      "Epoch 10/10\n",
      "6021/6021 [==============================] - 2s 321us/step - loss: 0.0523 - acc: 0.9822 - val_loss: 0.4964 - val_acc: 0.8537\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No such layer: flatten_4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3301da5061cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mlayer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'flatten_4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m intermediate_layer_model = Model(inputs=model.input,\n\u001b[0;32m---> 46\u001b[0;31m                                  outputs=model.get_layer(layer_name).output)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m    561\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such layer: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: flatten_4"
     ]
    }
   ],
   "source": [
    "#================== pakai yang ini ========================\n",
    "#maxlen 32; batch 16; fc 250  do-0.6 150 \n",
    "\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = max([len(x) for x in dataX])\n",
    "print(max_words)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu' ))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(100, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32, verbose=1)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# get embedding layer\n",
    "layer_name = 'flatten_4'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_train)\n",
    "\n",
    "\n",
    "model_json = intermediate_layer_model.to_json()\n",
    "with open(\"model_embedding.json\", \"w\") as json_file:\n",
    "\tjson_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "intermediate_layer_model.save_weights(\"model_embedding.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "numpy.savez('embeddedWord.npz', dataX = intermediate_output, dataY = dataY)\n",
    "print(\"data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(992,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "tweet = \"@ridwankamil Ya allah masih ada yang ngiri sama kang emil dan kota bandung nya 😂😂😂\"\n",
    "\n",
    "tweet = tweet.split()\n",
    "\n",
    "# cleaning\n",
    "import re\n",
    "for i in range(len(tweet)):\n",
    "    tweet[i] = re.sub(\"\\W\", \" \", tweet[i])\n",
    "    tweet[i] = re.sub(\"_\", \" \", tweet[i])\n",
    "    if re.search(r\"\\bhttp\\w+\", tweet[i]) != None:\n",
    "        tweet[i] = \"\"\n",
    "    if re.search(r\"\\d\", tweet[i]) != None:\n",
    "        tweet[i] = \"\"\n",
    "tweet = ' '.join(tweet)\n",
    "tweet = tweet.split()\n",
    "tweet = ' '.join(tweet) \n",
    "\n",
    "# convert to number\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "dict_file = open(\"dict.pickle\", \"rb\")\n",
    "char_to_int = pickle.load(dict_file)\n",
    "tweet = [char_to_int[char] for char in tweet.lower().split()]\n",
    "tweet = np.asarray(mytrain)\n",
    "tweet = tweet.reshape(1,tweet.shape[0])\n",
    "tweet = sequence.pad_sequences(tweet, maxlen=31)\n",
    "# tweet = tweet.reshape(1,31)\n",
    "\n",
    "# one hot vector\n",
    "json_file = open( 'model_embedding.json' ,  'r' )\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"model_embedding.h5\")\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "tweet = loaded_model.predict(tweet)\n",
    "\n",
    "print(np.asarray(tweet[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 250)               248250    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 273,653\n",
      "Trainable params: 273,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6021 samples, validate on 1506 samples\n",
      "Epoch 1/10\n",
      "6021/6021 [==============================] - 3s 459us/step - loss: 0.1848 - acc: 0.9234 - val_loss: 0.5704 - val_acc: 0.8251\n",
      "Epoch 2/10\n",
      "6021/6021 [==============================] - 2s 331us/step - loss: 0.0762 - acc: 0.9732 - val_loss: 0.7110 - val_acc: 0.8258\n",
      "Epoch 3/10\n",
      "6021/6021 [==============================] - 2s 316us/step - loss: 0.0648 - acc: 0.9793 - val_loss: 0.8236 - val_acc: 0.8220\n",
      "Epoch 4/10\n",
      "6021/6021 [==============================] - 2s 340us/step - loss: 0.0584 - acc: 0.9821 - val_loss: 0.8003 - val_acc: 0.8278\n",
      "Epoch 5/10\n",
      "6021/6021 [==============================] - 2s 339us/step - loss: 0.0569 - acc: 0.9819 - val_loss: 0.8936 - val_acc: 0.8187\n",
      "Epoch 6/10\n",
      "6021/6021 [==============================] - 2s 339us/step - loss: 0.0553 - acc: 0.9837 - val_loss: 0.7710 - val_acc: 0.8203\n",
      "Epoch 7/10\n",
      "6021/6021 [==============================] - 2s 345us/step - loss: 0.0524 - acc: 0.9844 - val_loss: 0.8956 - val_acc: 0.8236\n",
      "Epoch 8/10\n",
      "6021/6021 [==============================] - 2s 311us/step - loss: 0.0531 - acc: 0.9830 - val_loss: 0.8231 - val_acc: 0.8194\n",
      "Epoch 9/10\n",
      "6021/6021 [==============================] - 2s 359us/step - loss: 0.0487 - acc: 0.9859 - val_loss: 0.8511 - val_acc: 0.8245\n",
      "Epoch 10/10\n",
      "6021/6021 [==============================] - 2s 318us/step - loss: 0.0511 - acc: 0.9851 - val_loss: 0.8503 - val_acc: 0.8209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2b8cea90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maxlen 32; batch 16; fc 250  do-0.6 150 \n",
    "\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = max([len(x) for x in dataX])\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(250, activation= 'relu', input_shape=X_train[0].shape ))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(100, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'rmsprop' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=16, verbose=1)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7527, 992)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "acc: 95.56%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# ======= saving model ===============\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "\tjson_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# later...\n",
    "\n",
    "# load json and create model\n",
    "\n",
    "json_file = open( 'model.json' ,  'r' )\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import numpy\n",
    "data = numpy.load('embeddedWord.npz')\n",
    "dataX = data['dataX']\n",
    "dataY = data['dataY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        1.57106608e-01,  4.76610273e-01,  2.32316226e-01,  4.58666086e-02,\n",
       "        1.62675694e-01,  1.35985643e-01,  4.52228755e-01,  5.36736012e-01,\n",
       "        1.07958548e-01,  4.31194872e-01,  5.50123215e-01,  1.03685632e-01,\n",
       "        3.98033887e-01,  6.73777759e-02,  2.08472311e-01,  4.60713953e-01,\n",
       "        5.06911397e-01,  2.25148261e-01,  2.46028930e-01,  4.60814079e-03,\n",
       "        6.63964637e-03,  4.21138346e-01,  4.58641797e-01, -3.73029453e-03,\n",
       "        5.48258051e-02,  4.73444223e-01,  2.37630501e-01,  1.64919913e-01,\n",
       "        1.93461820e-01,  2.14828879e-01,  7.15295896e-02,  3.21918339e-01,\n",
       "        7.53401160e-01,  4.75615412e-01,  2.58032203e-01,  3.69112134e-01,\n",
       "        2.18343064e-01,  2.27209315e-01,  4.32410866e-01,  6.93425119e-01,\n",
       "        1.47163212e-01,  7.55399287e-01,  7.03956604e-01,  3.42410296e-01,\n",
       "       -4.47074361e-02,  1.20065853e-01,  2.18596905e-01,  7.83272535e-02,\n",
       "        8.26297462e-01,  3.80213350e-01,  3.20785403e-01,  6.16994441e-01,\n",
       "        4.49070364e-01,  1.54726058e-01,  3.23333919e-01,  9.82873261e-01,\n",
       "        1.48567706e-02,  4.89525832e-02,  8.24378610e-01,  7.64837861e-01,\n",
       "        9.93433833e-01,  2.25308135e-01,  9.05462980e-01,  4.84639592e-02,\n",
       "        7.42321849e-01,  2.04168782e-01,  7.07618237e-01,  8.30965400e-01,\n",
       "        3.13238710e-01,  3.22450370e-01,  9.79106009e-01,  3.33093613e-01,\n",
       "        6.98445618e-01,  6.48502469e-01,  1.32796794e-01,  6.00830138e-01,\n",
       "        3.69303375e-01,  1.97102904e-01,  5.52365363e-01,  4.05578285e-01,\n",
       "        3.10915887e-01,  8.64680409e-01,  9.12684441e-01,  5.05982161e-01,\n",
       "        7.41722345e-01,  6.24346852e-01,  7.97540367e-01,  1.36622995e-01,\n",
       "        3.66853386e-01,  8.24188113e-01,  6.42881334e-01,  1.07553267e+00,\n",
       "        1.45781979e-01,  4.30772126e-01,  1.53309897e-01,  3.38187039e-01,\n",
       "        1.00581959e-01, -2.10664663e-02, -4.82965149e-02, -2.27897521e-02,\n",
       "       -1.28249735e-01,  6.90310717e-01,  8.01709652e-01,  2.18674213e-01,\n",
       "        4.30194199e-01,  2.72745043e-02, -1.93374231e-02,  6.49277717e-02,\n",
       "        6.43999502e-02,  2.30723292e-01,  1.75338194e-01,  3.59300733e-01,\n",
       "        5.39741695e-01,  3.88471425e-01,  1.35507420e-01,  5.36569476e-01,\n",
       "        1.31761044e-01,  2.55574793e-01,  7.33349860e-01,  2.84405768e-01,\n",
       "       -2.30805993e-01,  2.68976331e-01,  1.98266909e-01,  4.73196507e-01,\n",
       "        1.99724451e-01,  2.64340460e-01,  1.59934849e-01,  8.30147862e-01,\n",
       "        4.05197144e-01,  1.04074001e+00,  5.03860652e-01,  4.05128002e-01,\n",
       "        7.33166516e-01,  7.88684905e-01,  2.26700664e-01,  1.06372073e-01,\n",
       "        5.77567160e-01,  8.46447110e-01, -1.10476814e-01, -8.16265941e-02,\n",
       "        4.69419897e-01,  1.06960079e-02,  6.25865340e-01,  7.22436249e-01,\n",
       "        4.68242764e-01,  7.08166480e-01,  7.75620282e-01,  6.11664116e-01,\n",
       "        1.01684535e+00,  8.64236951e-01,  5.55556178e-01,  6.58392608e-01,\n",
       "        2.14302316e-01,  5.01819253e-01,  3.27193648e-01,  3.17212120e-02,\n",
       "        1.00846708e+00,  3.46051008e-01,  3.70062590e-01,  3.25895220e-01,\n",
       "        3.31426263e-01,  4.95793253e-01,  1.15149462e+00,  5.84370136e-01,\n",
       "        3.69804353e-01,  4.64104295e-01,  5.53733885e-01,  3.26013535e-01,\n",
       "        6.03991866e-01,  5.71748972e-01,  1.00677323e+00,  1.30367011e-01,\n",
       "        7.88711786e-01,  2.27329120e-01,  9.45031047e-01,  4.83811885e-01,\n",
       "        8.73931885e-01,  7.03644380e-02,  7.45321929e-01,  5.25106490e-02,\n",
       "        2.25184619e-01,  8.67460608e-01,  6.51429474e-01,  9.57278490e-01,\n",
       "       -8.15283954e-02,  6.24531031e-01,  4.40054864e-01,  3.63967150e-01,\n",
       "        6.59628749e-01,  8.93881619e-01,  6.95902884e-01,  3.97993594e-01,\n",
       "        4.66363020e-02,  8.14892352e-01,  1.05186081e+00,  2.26458266e-01,\n",
       "        3.88716813e-03,  1.10133910e+00,  2.23941490e-01,  5.44116259e-01,\n",
       "        3.84601116e-01,  2.76726276e-01,  1.53134480e-01,  1.77324057e-01,\n",
       "        3.00987750e-01,  4.19719875e-01,  5.04298210e-01,  7.35689640e-01,\n",
       "        1.00044906e-01,  3.44513476e-01,  8.85271490e-01,  7.87480831e-01,\n",
       "        2.46786103e-01,  8.03452671e-01,  3.34688425e-01,  8.07292104e-01,\n",
       "        6.47757292e-01,  5.39005399e-01,  1.53966829e-01,  8.23367953e-01,\n",
       "        4.19157922e-01,  1.11827660e+00,  4.09883797e-01,  9.42840695e-01,\n",
       "        4.86412436e-01,  6.31491482e-01,  2.47794449e-01,  7.11029887e-01,\n",
       "        4.05090004e-02,  5.77697933e-01,  7.72068948e-02,  4.13072616e-01,\n",
       "        6.51676953e-01,  3.35360497e-01,  6.37544215e-01,  7.28770494e-01,\n",
       "        6.31611764e-01,  7.70035505e-01,  8.13862264e-01, -6.10029958e-02,\n",
       "        4.73161995e-01,  6.92423940e-01,  8.04306030e-01,  6.00299358e-01,\n",
       "        5.13706148e-01, -5.63478433e-02,  4.14198935e-01,  1.06038421e-01,\n",
       "        1.06226814e+00,  3.59165221e-01,  7.48833001e-01,  3.52063388e-01,\n",
       "        6.19566262e-01,  2.12391719e-01,  5.12281537e-01,  7.51873136e-01,\n",
       "       -9.76168513e-02,  6.04709268e-01,  9.73697126e-01,  3.08202177e-01,\n",
       "       -2.00300336e-01,  6.85875535e-01,  9.22220409e-01,  1.47223964e-01,\n",
       "        9.02611911e-02,  5.98514855e-01,  8.02571058e-01,  9.10325348e-01,\n",
       "       -2.64758579e-02,  1.19097538e-01,  4.37748700e-01,  6.81988299e-01,\n",
       "        8.40347230e-01,  1.98039323e-01,  9.67814028e-01, -7.14689493e-02,\n",
       "        3.09655517e-01,  3.11445713e-01,  3.86810511e-01,  2.61717349e-01,\n",
       "        1.03613541e-01,  5.10763049e-01,  2.52078176e-01,  3.71471822e-01,\n",
       "        2.36368142e-02,  4.52542543e-01,  7.30177760e-01,  9.22740102e-01,\n",
       "        6.20517731e-01,  1.54096289e-02,  1.54268578e-01,  4.40977275e-01,\n",
       "        9.96056259e-01,  2.39242077e-01,  1.22847050e-01,  5.89167237e-01,\n",
       "        4.66109663e-01,  6.06302500e-01,  3.61974239e-01,  3.69557679e-01,\n",
       "        7.66965866e-01,  3.58273745e-01,  4.54750925e-01,  9.82213914e-02,\n",
       "        5.40388077e-02,  1.93045102e-02,  2.00539544e-01,  6.39218152e-01,\n",
       "        5.62473714e-01,  5.53839087e-01, -2.16825455e-01,  7.18578756e-01,\n",
       "        1.31175792e+00,  7.22083986e-01,  7.48754680e-01,  8.86860430e-01,\n",
       "        4.01039720e-01,  2.30636448e-02,  1.06353569e+00,  1.07493687e+00,\n",
       "        1.02289237e-01,  3.72640431e-01,  3.59576553e-01,  7.78747022e-01,\n",
       "        9.06297266e-02,  2.85872668e-01,  1.25271678e-01,  4.85321224e-01,\n",
       "       -5.79285845e-02,  6.88823402e-01, -1.39994305e-02,  4.27310556e-01,\n",
       "        3.89018357e-01,  6.11540526e-02,  2.28519410e-01,  8.10838267e-02,\n",
       "        5.43580949e-01,  3.44641715e-01,  9.48540986e-01,  6.31002545e-01,\n",
       "        2.49661222e-01,  7.52587557e-01,  7.57590711e-01,  5.51590681e-01,\n",
       "        8.62904787e-01,  5.69714248e-01,  7.50448346e-01,  6.92389667e-01,\n",
       "        7.93359041e-01,  1.05490279e+00,  2.03401208e-01,  4.18900609e-01,\n",
       "        4.19960886e-01,  9.24806744e-02,  7.89820850e-01,  9.02993739e-01,\n",
       "        4.32617545e-01,  9.49031234e-01,  7.53020406e-01,  6.24385655e-01,\n",
       "        1.45548001e-01,  4.09522593e-01,  8.07616949e-01,  3.76814604e-01,\n",
       "        2.68239796e-01,  8.05501163e-01,  3.32454443e-01,  3.63713086e-01,\n",
       "        4.15719926e-01,  2.98978865e-01,  3.42120230e-01,  4.11477000e-01,\n",
       "        4.00391430e-01,  2.71528989e-01, -4.36049141e-02,  6.27576351e-01,\n",
       "        2.37791359e-01,  2.07624093e-01,  1.44593120e-01,  7.35192597e-01,\n",
       "        2.62330115e-01,  4.51996028e-01,  7.81512082e-01,  4.43844199e-02,\n",
       "        8.21204782e-01,  1.06309557e+00,  1.33566886e-01,  5.27829528e-01,\n",
       "        7.80021071e-01,  6.16202772e-01,  3.14223468e-01,  3.21611136e-01,\n",
       "        4.04418707e-01,  7.33648419e-01,  2.86589921e-01,  1.85750753e-01,\n",
       "       -1.35967165e-01,  2.38920450e-01,  9.48207006e-02,  2.29161426e-01,\n",
       "        5.00398159e-01,  7.04371452e-01,  6.20918214e-01,  4.71394844e-02,\n",
       "        6.77513003e-01,  2.69199163e-01,  4.28584099e-01,  8.88198614e-01,\n",
       "        4.17148352e-01,  7.43243575e-01,  9.42494452e-01,  7.04791129e-01,\n",
       "        8.38937700e-01,  5.70676982e-01,  5.09602547e-01,  9.05856133e-01,\n",
       "        4.66363020e-02,  8.14892352e-01,  1.05186081e+00,  2.26458266e-01,\n",
       "        3.88716813e-03,  1.10133910e+00,  2.23941490e-01,  5.44116259e-01,\n",
       "        3.84601116e-01,  2.76726276e-01,  1.53134480e-01,  1.77324057e-01,\n",
       "        3.00987750e-01,  4.19719875e-01,  5.04298210e-01,  7.35689640e-01,\n",
       "        1.00044906e-01,  3.44513476e-01,  8.85271490e-01,  7.87480831e-01,\n",
       "        2.46786103e-01,  8.03452671e-01,  3.34688425e-01,  8.07292104e-01,\n",
       "        6.47757292e-01,  5.39005399e-01,  1.53966829e-01,  8.23367953e-01,\n",
       "        4.19157922e-01,  1.11827660e+00,  4.09883797e-01,  9.42840695e-01,\n",
       "        2.81278074e-01,  2.34930784e-01,  3.42363656e-01,  8.86177242e-01,\n",
       "        2.99687147e-01,  7.60319889e-01,  6.49445653e-01,  8.72878969e-01,\n",
       "        5.39913356e-01,  4.75945830e-01,  7.29920685e-01,  1.63005784e-01,\n",
       "        7.34758377e-01, -3.17934416e-02,  6.28417969e-01,  6.76556885e-01,\n",
       "        4.47761297e-01,  2.08676919e-01,  7.22023726e-01,  1.05762732e+00,\n",
       "        8.12656879e-01,  8.04369688e-01,  6.11430526e-01,  4.11896050e-01,\n",
       "        4.98012662e-01,  7.38674939e-01,  6.43795550e-01,  5.93677700e-01,\n",
       "        9.73006710e-04,  6.02315724e-01,  1.04692531e+00,  1.32225957e-02,\n",
       "        2.15267181e-01,  3.05669188e-01, -5.45779355e-02,  8.98233294e-01,\n",
       "        9.74964321e-01, -5.10819592e-02,  5.07999659e-01,  3.10870767e-01,\n",
       "       -9.82567109e-03,  2.15613708e-01,  7.78373003e-01,  9.32074010e-01,\n",
       "        8.07204604e-01,  7.95904279e-01,  3.24908972e-01,  6.36240721e-01,\n",
       "        6.58863127e-01,  3.16316277e-01,  6.97986782e-01,  8.39730501e-01,\n",
       "        4.42158490e-01,  3.80419970e-01,  7.58129239e-01,  1.15741603e-01,\n",
       "        8.41670513e-01,  6.16847575e-01,  6.77057564e-01, -2.37377640e-03,\n",
       "        5.70939422e-01,  5.84056318e-01,  6.19106710e-01,  5.33409238e-01,\n",
       "        3.23950499e-01,  3.70151103e-01,  6.59485579e-01,  2.96678007e-01,\n",
       "        8.42025042e-01,  1.13110421e-02,  2.15675384e-01,  5.55569410e-01,\n",
       "        9.59040046e-01,  2.45741069e-01,  3.34396034e-01,  5.01611471e-01,\n",
       "        6.39945447e-01,  3.11841786e-01,  4.98023361e-01,  5.32881737e-01,\n",
       "        1.32809103e-01,  2.58635670e-01,  4.75073248e-01,  4.83219504e-01,\n",
       "        4.48092431e-01, -3.59726511e-02,  5.71221948e-01, -7.27085546e-02,\n",
       "        1.05148399e+00,  5.71375847e-01,  8.67740095e-01,  6.71275675e-01,\n",
       "        5.83419740e-01,  1.48644432e-01,  4.70828474e-01,  1.44720063e-01,\n",
       "        8.32596004e-01,  5.20024538e-01,  1.01084912e+00,  5.45750499e-01,\n",
       "        1.89683028e-02,  6.97911441e-01,  2.09433362e-01,  6.79383874e-01,\n",
       "        3.07929605e-01,  3.29869807e-01,  3.59209716e-01,  2.28512123e-01,\n",
       "        3.30671310e-01,  5.57089567e-01,  7.71255195e-01,  8.24262679e-01,\n",
       "        4.77132052e-01,  1.87762573e-01,  3.08457464e-01,  7.03086317e-01,\n",
       "        5.37476897e-01,  3.16886961e-01,  8.33485663e-01,  8.97711396e-01,\n",
       "        1.06412590e+00,  8.69231522e-02,  1.10279989e+00,  1.98655277e-01,\n",
       "        3.73347610e-01,  6.20180070e-01,  5.00655890e-01,  7.35310912e-01,\n",
       "        2.03961700e-01,  5.69061279e-01,  9.80811417e-01,  9.80349541e-01,\n",
       "        9.35930014e-01,  3.88368100e-01,  7.53363609e-01,  1.37920067e-01,\n",
       "       -1.24123856e-01,  2.94208497e-01, -1.16617985e-01,  9.18738782e-01,\n",
       "        2.04330683e-01,  2.50917107e-01,  9.31042314e-01,  1.06144536e+00,\n",
       "        4.87231672e-01,  6.23761475e-01,  1.10588171e-01,  8.70499671e-01,\n",
       "        2.84328163e-01,  4.84477788e-01,  1.66174993e-01,  5.21836281e-01,\n",
       "        6.56746209e-01,  4.54137146e-01,  6.72189951e-01,  5.74462533e-01,\n",
       "        7.15742767e-01,  5.62111676e-01,  1.04513919e+00,  8.42998147e-01,\n",
       "        4.31864589e-01,  4.48194683e-01,  1.08246791e+00,  3.77897203e-01,\n",
       "        7.66501650e-02,  5.57972252e-01,  7.59230852e-01,  6.76891387e-01,\n",
       "        3.36924225e-01,  4.11563814e-01,  1.79484054e-01,  6.23079300e-01,\n",
       "       -4.20821719e-02,  5.33260345e-01,  5.88313222e-01,  7.25425661e-01,\n",
       "        3.96126658e-01,  8.70718598e-01,  1.42017245e-01,  5.10468602e-01,\n",
       "        1.93135902e-01,  4.47496623e-01,  2.15780228e-01,  5.46990447e-02,\n",
       "        4.38410521e-01,  2.16605917e-01,  4.28311199e-01,  6.54850900e-01,\n",
       "        6.77102149e-01,  3.51426601e-01,  1.33196980e-01,  5.48677921e-01,\n",
       "        7.15536654e-01,  6.53890610e-01,  4.03259337e-01,  4.76420522e-01,\n",
       "        3.20332795e-01,  6.76321805e-01,  7.98957288e-01,  2.72842079e-01,\n",
       "        8.53448212e-01,  3.83702785e-01,  8.34191859e-01,  5.61401725e-01,\n",
       "        3.62709790e-01,  2.19903484e-01,  5.84622979e-01,  9.61278498e-01,\n",
       "        6.50950372e-01,  5.72341263e-01,  8.06571841e-02,  1.07893395e+00,\n",
       "        8.15418884e-02,  3.10319752e-01,  7.15626776e-01,  8.20391178e-01,\n",
       "        1.73779756e-01,  5.46895206e-01,  3.90655488e-01,  1.08681694e-01,\n",
       "        7.91594088e-01,  3.14935654e-01, -1.83183104e-02,  1.27151445e-01,\n",
       "        9.24199879e-01,  1.55039921e-01, -1.30874112e-01,  9.58464667e-02,\n",
       "        9.03131843e-01,  2.08971441e-01,  2.66984999e-01,  3.58158201e-01,\n",
       "       -7.10178986e-02,  4.51168060e-01,  2.02035531e-02,  8.48191381e-01,\n",
       "        1.09485509e-02,  1.08012116e+00, -1.24556408e-03,  9.96858120e-01,\n",
       "        2.67922014e-01,  1.01711559e+00,  8.65054548e-01, -5.21780998e-02,\n",
       "        6.48632824e-01,  5.82839251e-01,  8.36739182e-01,  3.72535437e-01,\n",
       "        6.25564992e-01,  1.09664679e-01,  3.55713159e-01,  1.92535579e-01,\n",
       "        4.06298220e-01,  4.39540356e-01,  3.99244100e-01,  6.86214089e-01,\n",
       "        8.96002650e-02,  3.41908902e-01,  5.80846488e-01,  7.22808301e-01,\n",
       "        4.87290829e-01,  2.37954691e-01,  3.39489400e-01,  4.97210503e-01,\n",
       "        2.23941982e-01,  2.39355803e-01,  7.96989381e-01,  3.17538589e-01,\n",
       "        1.62135571e-01,  8.39106381e-01,  1.04100943e+00,  3.44693601e-01,\n",
       "        3.04934442e-01,  7.81156838e-01,  1.19525559e-01,  2.51362976e-02,\n",
       "        9.28592265e-01,  5.91677487e-01,  2.16996372e-01,  3.45609874e-01,\n",
       "        6.97068647e-02,  6.29567206e-01,  1.13824658e-01,  5.68159938e-01,\n",
       "        5.55161059e-01,  5.94163477e-01,  4.73601937e-01,  3.75271529e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['dataX'][0]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "\n",
    "dataY_svm = []\n",
    "for i in range(len(dataY)):\n",
    "    dataY_svm.append(numpy.argmax(dataY[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 96.01 %\n"
     ]
    }
   ],
   "source": [
    "# ==============     SVM    ======================\n",
    "\n",
    "\n",
    "\n",
    "# Train model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "# score = cross_val_score(model, dataX, dataY_svm, scoring='accuracy', cv=5)\n",
    "# print(score, numpy.mean(score))\n",
    "\n",
    "model.fit(dataX, dataY_svm)\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "model_filename = \"model_svm.pkl\"  \n",
    "with open(model_filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load from file\n",
    "with open(model_filename, 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "\n",
    "score = model.score(dataX, dataY_svm)  \n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 25.99 %\n"
     ]
    }
   ],
   "source": [
    "# =============== Naive Bayes ===================\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(dataX, dataY_svm)\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "model_filename = \"model_naive.pkl\"  \n",
    "with open(model_filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load from file\n",
    "with open(model_filename, 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "\n",
    "score = model.score(dataX, dataY_svm)  \n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 99.76 %\n"
     ]
    }
   ],
   "source": [
    "# =============== Random Forest ===================\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "model.fit(dataX, dataY_svm)\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "model_filename = \"model_RForest.pkl\"  \n",
    "with open(model_filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "\n",
    "# Load from file\n",
    "with open(model_filename, 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "\n",
    "score = model.score(dataX, dataY_svm)  \n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Model loaded\n",
      "ANN acc: 95.56%\n",
      "Random forest model loaded\n",
      "Random Forest acc: 99.76 %\n",
      "SVM model loaded\n",
      "SMV acc: 96.01 %\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "#load data\n",
    "data = numpy.load('embeddedWord.npz')\n",
    "dataX = data['dataX']\n",
    "dataY = data['dataY']\n",
    "\n",
    "# ============= ANN =====================\n",
    "\n",
    "json_file = open( 'model.json' ,  'r' )\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"ANN Model loaded\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"ANN %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "\n",
    "# prep target for other model\n",
    "\n",
    "dataY_svm = []\n",
    "for i in range(len(dataY)):\n",
    "    dataY_svm.append(numpy.argmax(dataY[i]))\n",
    "\n",
    "\n",
    "# ============= Random Forest ==================\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"model_RForest.pkl\", 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "print(\"Random forest model loaded\")\n",
    "    \n",
    "score = model.score(dataX, dataY_svm)  \n",
    "print(\"Random Forest acc: {0:.2f} %\".format(100 * score))\n",
    "\n",
    "\n",
    "\n",
    "# ============= SVM ==================\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"model_svm.pkl\", 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "print(\"SVM model loaded\")\n",
    "\n",
    "score = model.score(dataX, dataY_svm)  \n",
    "print(\"SMV acc: {0:.2f} %\".format(100 * score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating one hot vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 31, 10)            100640    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 310)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 933       \n",
      "=================================================================\n",
      "Total params: 101,573\n",
      "Trainable params: 101,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Data saved !\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = max([len(x) for x in dataX])\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, 10, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "\n",
    "# get embedding layer\n",
    "layer_name = 'flatten_2'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_train)\n",
    "\n",
    "numpy.savez('embeddedWord.npz', dataX = intermediate_output, dataY = dataY)\n",
    "print('Data saved !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Experiment with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           352000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 753       \n",
      "=================================================================\n",
      "Total params: 4,415,753\n",
      "Trainable params: 4,415,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayatura/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6021 samples, validate on 1506 samples\n",
      "Epoch 1/10\n",
      "6021/6021 [==============================] - 28s 5ms/step - loss: 2.1790 - acc: 0.6554 - val_loss: 0.5508 - val_acc: 0.6662\n",
      "Epoch 2/10\n",
      "6021/6021 [==============================] - 26s 4ms/step - loss: 0.5315 - acc: 0.6852 - val_loss: 0.4611 - val_acc: 0.8008\n",
      "Epoch 3/10\n",
      "6021/6021 [==============================] - 26s 4ms/step - loss: 0.4513 - acc: 0.7268 - val_loss: 0.4409 - val_acc: 0.7833\n",
      "Epoch 4/10\n",
      "6021/6021 [==============================] - 28s 5ms/step - loss: 0.3670 - acc: 0.8071 - val_loss: 0.4024 - val_acc: 0.7911\n",
      "Epoch 5/10\n",
      "6021/6021 [==============================] - 27s 4ms/step - loss: 0.2352 - acc: 0.9053 - val_loss: 0.9085 - val_acc: 0.6228\n",
      "Epoch 6/10\n",
      "6021/6021 [==============================] - 26s 4ms/step - loss: 0.1183 - acc: 0.9603 - val_loss: 0.6063 - val_acc: 0.7897\n",
      "Epoch 7/10\n",
      "6021/6021 [==============================] - 27s 4ms/step - loss: 0.0692 - acc: 0.9813 - val_loss: 0.6184 - val_acc: 0.7926\n",
      "Epoch 8/10\n",
      "6021/6021 [==============================] - 27s 4ms/step - loss: 0.0511 - acc: 0.9866 - val_loss: 0.7489 - val_acc: 0.7793\n",
      "Epoch 9/10\n",
      "6021/6021 [==============================] - 28s 5ms/step - loss: 0.0419 - acc: 0.9889 - val_loss: 0.7201 - val_acc: 0.7908\n",
      "Epoch 10/10\n",
      "6021/6021 [==============================] - 27s 5ms/step - loss: 0.0365 - acc: 0.9900 - val_loss: 0.8433 - val_acc: 0.7634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb29224198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(11000, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu' ))\n",
    "model.add(Dense(250, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=16, verbose=1)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           352000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 4,377,653\n",
      "Trainable params: 4,377,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayatura/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6021 samples, validate on 1506 samples\n",
      "Epoch 1/10\n",
      "6021/6021 [==============================] - 8s 1ms/step - loss: 1.3957 - acc: 0.6399 - val_loss: 1.3104 - val_acc: 0.5002\n",
      "Epoch 2/10\n",
      "6021/6021 [==============================] - 8s 1ms/step - loss: 1.1565 - acc: 0.6961 - val_loss: 0.9861 - val_acc: 0.5494\n",
      "Epoch 3/10\n",
      "6021/6021 [==============================] - 8s 1ms/step - loss: 1.1098 - acc: 0.7439 - val_loss: 0.7921 - val_acc: 0.7758\n",
      "Epoch 4/10\n",
      "6021/6021 [==============================] - 9s 1ms/step - loss: 1.0329 - acc: 0.8084 - val_loss: 0.7876 - val_acc: 0.7711\n",
      "Epoch 5/10\n",
      "6021/6021 [==============================] - 9s 1ms/step - loss: 0.9756 - acc: 0.8372 - val_loss: 0.7248 - val_acc: 0.6720\n",
      "Epoch 6/10\n",
      "6021/6021 [==============================] - 8s 1ms/step - loss: 0.3039 - acc: 0.8764 - val_loss: 0.4733 - val_acc: 0.7598\n",
      "Epoch 7/10\n",
      "6021/6021 [==============================] - 9s 2ms/step - loss: 0.2031 - acc: 0.9171 - val_loss: 0.4055 - val_acc: 0.7986\n",
      "Epoch 8/10\n",
      "6021/6021 [==============================] - 8s 1ms/step - loss: 0.1301 - acc: 0.9552 - val_loss: 0.4904 - val_acc: 0.7973\n",
      "Epoch 9/10\n",
      "6021/6021 [==============================] - 9s 1ms/step - loss: 0.1043 - acc: 0.9647 - val_loss: 0.4299 - val_acc: 0.8444\n",
      "Epoch 10/10\n",
      "6021/6021 [==============================] - 13s 2ms/step - loss: 0.0777 - acc: 0.9758 - val_loss: 0.7625 - val_acc: 0.6791\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb29844940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(11000, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu' ))\n",
    "model.add(Dense(100, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=64, verbose=1)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 31, 32)            352000    \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 250)               248250    \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 625,653\n",
      "Trainable params: 625,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayatura/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6021 samples, validate on 1506 samples\n",
      "Epoch 1/10\n",
      "6021/6021 [==============================] - 2s 395us/step - loss: 0.5150 - acc: 0.7175 - val_loss: 0.4836 - val_acc: 0.6815\n",
      "Epoch 2/10\n",
      "6021/6021 [==============================] - 1s 177us/step - loss: 0.3805 - acc: 0.8106 - val_loss: 0.4910 - val_acc: 0.6985\n",
      "Epoch 3/10\n",
      "6021/6021 [==============================] - 1s 190us/step - loss: 0.2700 - acc: 0.8802 - val_loss: 0.4420 - val_acc: 0.7652\n",
      "Epoch 4/10\n",
      "6021/6021 [==============================] - 1s 186us/step - loss: 0.1623 - acc: 0.9354 - val_loss: 0.3805 - val_acc: 0.8490\n",
      "Epoch 5/10\n",
      "6021/6021 [==============================] - 1s 203us/step - loss: 0.0973 - acc: 0.9647 - val_loss: 0.4541 - val_acc: 0.8012\n",
      "Epoch 6/10\n",
      "6021/6021 [==============================] - 1s 195us/step - loss: 0.0635 - acc: 0.9808 - val_loss: 0.5093 - val_acc: 0.8006\n",
      "Epoch 7/10\n",
      "6021/6021 [==============================] - 1s 218us/step - loss: 0.0413 - acc: 0.9877 - val_loss: 0.4910 - val_acc: 0.8063\n",
      "Epoch 8/10\n",
      "6021/6021 [==============================] - 1s 204us/step - loss: 0.0250 - acc: 0.9938 - val_loss: 0.7450 - val_acc: 0.7727\n",
      "Epoch 9/10\n",
      "6021/6021 [==============================] - 1s 211us/step - loss: 0.0230 - acc: 0.9937 - val_loss: 0.6925 - val_acc: 0.7831\n",
      "Epoch 10/10\n",
      "6021/6021 [==============================] - 1s 190us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.4949 - val_acc: 0.8165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb52316f98>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maxlen 32; batch 64; fc 250 100 \n",
    "\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = max([len(x) for x in dataX])\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(11000, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu' ))\n",
    "model.add(Dense(100, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=64, verbose=1)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 31, 32)            352000    \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 250)               248250    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 625,653\n",
      "Trainable params: 625,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayatura/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6021 samples, validate on 1506 samples\n",
      "Epoch 1/10\n",
      "6021/6021 [==============================] - 5s 820us/step - loss: 0.4617 - acc: 0.7414 - val_loss: 0.4450 - val_acc: 0.7081\n",
      "Epoch 2/10\n",
      "6021/6021 [==============================] - 4s 605us/step - loss: 0.2707 - acc: 0.8777 - val_loss: 0.6940 - val_acc: 0.6669\n",
      "Epoch 3/10\n",
      "6021/6021 [==============================] - 4s 619us/step - loss: 0.1303 - acc: 0.9490 - val_loss: 0.4487 - val_acc: 0.7441\n",
      "Epoch 4/10\n",
      "6021/6021 [==============================] - 4s 604us/step - loss: 0.0641 - acc: 0.9789 - val_loss: 0.3876 - val_acc: 0.8628\n",
      "Epoch 5/10\n",
      "6021/6021 [==============================] - 4s 597us/step - loss: 0.0377 - acc: 0.9885 - val_loss: 0.4579 - val_acc: 0.8178\n",
      "Epoch 6/10\n",
      "6021/6021 [==============================] - 4s 626us/step - loss: 0.0253 - acc: 0.9931 - val_loss: 0.4658 - val_acc: 0.8404\n",
      "Epoch 7/10\n",
      "6021/6021 [==============================] - 4s 621us/step - loss: 0.0216 - acc: 0.9937 - val_loss: 0.4854 - val_acc: 0.8632\n",
      "Epoch 8/10\n",
      "6021/6021 [==============================] - 4s 697us/step - loss: 0.0196 - acc: 0.9946 - val_loss: 1.3407 - val_acc: 0.7107\n",
      "Epoch 9/10\n",
      "6021/6021 [==============================] - 4s 637us/step - loss: 0.0173 - acc: 0.9952 - val_loss: 0.9714 - val_acc: 0.7503\n",
      "Epoch 10/10\n",
      "6021/6021 [==============================] - 4s 620us/step - loss: 0.0156 - acc: 0.9954 - val_loss: 0.7420 - val_acc: 0.7494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10e55d978>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maxlen 32; batch 16; fc 250 100 \n",
    "\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = max([len(x) for x in dataX])\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(11000, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation= 'relu' ))\n",
    "model.add(Dense(100, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=16, verbose=1)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 31, 32)            352000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               248250    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               25100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 625,653\n",
      "Trainable params: 625,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayatura/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3763 samples, validate on 3764 samples\n",
      "Epoch 1/10\n",
      "3763/3763 [==============================] - 2s 510us/step - loss: 0.5587 - acc: 0.6750 - val_loss: 0.5340 - val_acc: 0.7050\n",
      "Epoch 2/10\n",
      "3763/3763 [==============================] - 1s 367us/step - loss: 0.4816 - acc: 0.7283 - val_loss: 0.4872 - val_acc: 0.7419\n",
      "Epoch 3/10\n",
      "3763/3763 [==============================] - 1s 346us/step - loss: 0.4152 - acc: 0.7796 - val_loss: 0.4647 - val_acc: 0.7593\n",
      "Epoch 4/10\n",
      "3763/3763 [==============================] - 1s 352us/step - loss: 0.3487 - acc: 0.8317 - val_loss: 0.4751 - val_acc: 0.7642\n",
      "Epoch 5/10\n",
      "3763/3763 [==============================] - 1s 347us/step - loss: 0.2790 - acc: 0.8769 - val_loss: 0.4310 - val_acc: 0.7855\n",
      "Epoch 6/10\n",
      "3763/3763 [==============================] - 1s 353us/step - loss: 0.2067 - acc: 0.9135 - val_loss: 0.4984 - val_acc: 0.7683\n",
      "Epoch 7/10\n",
      "3763/3763 [==============================] - 1s 396us/step - loss: 0.1556 - acc: 0.9395 - val_loss: 0.4673 - val_acc: 0.8131\n",
      "Epoch 8/10\n",
      "3763/3763 [==============================] - 1s 355us/step - loss: 0.1239 - acc: 0.9555 - val_loss: 0.5020 - val_acc: 0.7682\n",
      "Epoch 9/10\n",
      "3763/3763 [==============================] - 1s 381us/step - loss: 0.0919 - acc: 0.9657 - val_loss: 0.5332 - val_acc: 0.7828\n",
      "Epoch 10/10\n",
      "3763/3763 [==============================] - 1s 390us/step - loss: 0.0673 - acc: 0.9771 - val_loss: 0.6030 - val_acc: 0.8206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2dbb3978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maxlen 32; batch 16; fc 250 250 \n",
    "\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = max([len(x) for x in dataX])\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(11000, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150, activation= 'relu' ))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(100, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.5, batch_size=32, verbose=1)\n",
    "# Final evaluation of the model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 31, 32)            352000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 992)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               99300     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 461,703\n",
      "Trainable params: 461,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dayatura/tensorflow3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3763 samples, validate on 3764 samples\n",
      "Epoch 1/10\n",
      "3763/3763 [==============================] - 2s 401us/step - loss: 0.5713 - acc: 0.6700 - val_loss: 0.5520 - val_acc: 0.6823\n",
      "Epoch 2/10\n",
      "3763/3763 [==============================] - 1s 265us/step - loss: 0.5151 - acc: 0.6956 - val_loss: 0.5201 - val_acc: 0.7453\n",
      "Epoch 3/10\n",
      "3763/3763 [==============================] - 1s 250us/step - loss: 0.4655 - acc: 0.7491 - val_loss: 0.4747 - val_acc: 0.7470\n",
      "Epoch 4/10\n",
      "3763/3763 [==============================] - 1s 258us/step - loss: 0.4160 - acc: 0.7850 - val_loss: 0.4801 - val_acc: 0.7703\n",
      "Epoch 5/10\n",
      "3763/3763 [==============================] - 1s 281us/step - loss: 0.3377 - acc: 0.8448 - val_loss: 0.4583 - val_acc: 0.7855\n",
      "Epoch 6/10\n",
      "3763/3763 [==============================] - 1s 308us/step - loss: 0.2861 - acc: 0.8786 - val_loss: 0.4798 - val_acc: 0.7982\n",
      "Epoch 7/10\n",
      "3763/3763 [==============================] - 1s 286us/step - loss: 0.2308 - acc: 0.9057 - val_loss: 0.4741 - val_acc: 0.8036\n",
      "Epoch 8/10\n",
      "3763/3763 [==============================] - 1s 260us/step - loss: 0.1783 - acc: 0.9286 - val_loss: 0.4945 - val_acc: 0.8084\n",
      "Epoch 9/10\n",
      "3763/3763 [==============================] - 1s 257us/step - loss: 0.1533 - acc: 0.9393 - val_loss: 0.4930 - val_acc: 0.8068\n",
      "Epoch 10/10\n",
      "3763/3763 [==============================] - 1s 283us/step - loss: 0.1409 - acc: 0.9435 - val_loss: 0.6178 - val_acc: 0.8100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb2a2c30b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#maxlen 32; batch 32; fc 100 100 \n",
    "\n",
    "\n",
    "import numpy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "# top_words = 5000\n",
    "X_train = dataX\n",
    "y_train = dataY\n",
    "\n",
    "max_words = max([len(x) for x in dataX])\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(11000, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation= 'relu' ))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(100, activation= 'relu'))\n",
    "model.add(Dense(3, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "print(model.summary())\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_split=0.5, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF Python 3",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
